{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40770339",
   "metadata": {},
   "source": [
    "# Employee Attrition Prediction\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow to predict employee attrition using the **IBM HR Analytics Employee Attrition & Performance** dataset.\n",
    "\n",
    "**What this notebook includes**\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Feature engineering and preprocessing\n",
    "- Handling class imbalance with SMOTE\n",
    "- Training and evaluation of **Random Forest** and **XGBoost**\n",
    "- Feature importance and saving the best model\n",
    "\n",
    "> Place the dataset CSV file as `data/WA_Fn-UseC_-HR-Employee-Attrition.csv` before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97680199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "print(\"Imports loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = os.path.join('..', 'data', 'WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Dataset not found at {data_path}. Please download the dataset from Kaggle and place it in the data/ folder.\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"Loaded dataset shape:\", df.shape)\n",
    "    display(df.head(5))\n",
    "    display(df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "print(\"Attrition value counts:\")\n",
    "print(df['Attrition'].value_counts(), '\\n')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Attrition', data=df)\n",
    "plt.title('Attrition Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Numeric correlation heatmap\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df[num_cols].corr(), cmap='coolwarm', center=0)\n",
    "plt.title('Correlation matrix (numeric features)')\n",
    "plt.show()\n",
    "\n",
    "# OverTime vs Attrition\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='OverTime', hue='Attrition', data=df)\n",
    "plt.title('OverTime vs Attrition')\n",
    "plt.show()\n",
    "\n",
    "# JobSatisfaction vs Attrition\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='JobSatisfaction', hue='Attrition', data=df)\n",
    "plt.title('Job Satisfaction vs Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23896007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df = df.copy()\n",
    "\n",
    "# Map target to binary\n",
    "df['Attrition'] = df['Attrition'].map({'Yes':1, 'No':0})\n",
    "\n",
    "# Check missing values\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Top missing (if any):\")\n",
    "print(missing[missing>0] if missing.sum()>0 else \"No missing values found\")\n",
    "\n",
    "# Encode categorical variables using LabelEncoder (simple and reproducible)\n",
    "cat_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('Attrition', axis=1)\n",
    "y = df['Attrition']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84090a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train class distribution (before SMOTE):\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"Train class distribution (after SMOTE):\")\n",
    "print(pd.Series(y_train_res).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest training and evaluation\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy: {:.4f}\".format(acc_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Random Forest - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# RF ROC AUC\n",
    "y_proba_rf = rf.predict_proba(X_test)[:,1]\n",
    "print(\"Random Forest ROC AUC: {:.4f}\".format(roc_auc_score(y_test, y_proba_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd239f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost training and evaluation\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_res, y_train_res)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Accuracy: {:.4f}\".format(acc_xgb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('XGBoost - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# XGBoost ROC AUC\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)[:,1]\n",
    "print(\"XGBoost ROC AUC: {:.4f}\".format(roc_auc_score(y_test, y_proba_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance - XGBoost\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    xgb.plot_importance(xgb_model, max_num_features=15, importance_type='gain', ax=ax)\n",
    "    plt.title('XGBoost - Feature Importance (gain)')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Could not plot XGBoost importance:\", e)\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head(15)\n",
    "plt.figure(figsize=(8,6))\n",
    "rf_imp.plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Random Forest - Top 15 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60afaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "os.makedirs(os.path.join('..','models'), exist_ok=True)\n",
    "\n",
    "if acc_xgb >= acc_rf:\n",
    "    best_model = xgb_model\n",
    "    best_name = 'xgboost'\n",
    "    best_acc = acc_xgb\n",
    "else:\n",
    "    best_model = rf\n",
    "    best_name = 'random_forest'\n",
    "    best_acc = acc_rf\n",
    "\n",
    "model_path = os.path.join('..', 'models', f'best_model_{best_name}.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"Saved best model ({best_name}) with accuracy {best_acc:.4f} to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on a sample from the test set\n",
    "sample = X_test.iloc[0:3]\n",
    "probs = best_model.predict_proba(sample)[:,1]\n",
    "preds = best_model.predict(sample)\n",
    "print(\"Predicted probabilities (attrition):\", probs)\n",
    "print(\"Predicted classes:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1547ab7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next steps / Notes\n",
    "\n",
    "- If you want to expose this as an API, you can create a small Flask app that loads `models/best_model_*.pkl` and returns predictions for input employee records.\n",
    "- To reproduce results, ensure you use the same `data/` CSV and the required packages in `requirements.txt`.\n",
    "- If you want, I can also generate a `app.py` (Flask) and a `Procfile` for Heroku deployment.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
